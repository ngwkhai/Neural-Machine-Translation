{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12201813,"sourceType":"datasetVersion","datasetId":7685807}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport re\nimport string\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T03:50:12.602650Z","iopub.execute_input":"2025-06-30T03:50:12.603216Z","iopub.status.idle":"2025-06-30T03:50:30.559650Z","shell.execute_reply.started":"2025-06-30T03:50:12.603192Z","shell.execute_reply":"2025-06-30T03:50:30.558981Z"}},"outputs":[{"name":"stderr","text":"2025-06-30 03:50:18.476069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751255418.680072      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751255418.740808      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- DEVICE SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T03:50:30.560837Z","iopub.execute_input":"2025-06-30T03:50:30.561367Z","iopub.status.idle":"2025-06-30T03:50:30.645340Z","shell.execute_reply.started":"2025-06-30T03:50:30.561341Z","shell.execute_reply":"2025-06-30T03:50:30.644533Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- DATA CLEANING ---\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# --- LOAD & PREPROCESS ---\npath = \"/kaggle/input/vietnamese-diacritics-dataset/\"\ndf_train = pd.read_csv(path + \"ViDiacritics_train.csv\")\ndf_val   = pd.read_csv(path + \"ViDiacritics_val.csv\")\ndf_test  = pd.read_csv(path + \"ViDiacritics_test.csv\")\n\n# sample for speed\ndf_train = df_train.sample(frac=0.01, random_state=42).reset_index(drop=True)\ndf_val   = df_val.sample(frac=0.01, random_state=42).reset_index(drop=True)\ndf_test  = df_test.sample(frac=0.01, random_state=42).reset_index(drop=True)\n\n# apply cleaning and add tokens\nfor df in [df_train, df_val, df_test]:\n    df['no_diacritics_clean']   = df['no_diacritics'].astype(str).apply(clean_text)\n    df['with_diacritics_clean'] = df['with_diacritics'].astype(str).apply(clean_text)\n    df['with_diacritics_clean'] = df['with_diacritics_clean'].apply(lambda x: '<start> ' + x + ' <end>')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T03:50:30.646291Z","iopub.execute_input":"2025-06-30T03:50:30.646593Z","iopub.status.idle":"2025-06-30T03:51:46.826225Z","shell.execute_reply.started":"2025-06-30T03:50:30.646545Z","shell.execute_reply":"2025-06-30T03:51:46.825613Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- TOKENIZATION ---\nfilters = '\"!#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # preserve <> for start/end\nsrc_tokenizer = Tokenizer(oov_token='<unk>', filters=filters)\ntgt_tokenizer = Tokenizer(oov_token='<unk>', filters=filters)\nsrc_tokenizer.fit_on_texts(df_train['no_diacritics_clean'])\ntgt_tokenizer.fit_on_texts(df_train['with_diacritics_clean'])\n\nSRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\nTGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\nMAX_LEN = 70","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T03:51:46.828523Z","iopub.execute_input":"2025-06-30T03:51:46.828754Z","iopub.status.idle":"2025-06-30T03:51:50.133354Z","shell.execute_reply.started":"2025-06-30T03:51:46.828736Z","shell.execute_reply":"2025-06-30T03:51:50.132577Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# encode & pad\ndef encode_and_pad(texts, tokenizer):\n    seqs = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post')\n\ntrain_src = encode_and_pad(df_train['no_diacritics_clean'], src_tokenizer)\ntrain_tgt = encode_and_pad(df_train['with_diacritics_clean'], tgt_tokenizer)\nval_src   = encode_and_pad(df_val['no_diacritics_clean'], src_tokenizer)\nval_tgt   = encode_and_pad(df_val['with_diacritics_clean'], tgt_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T03:51:50.134180Z","iopub.execute_input":"2025-06-30T03:51:50.134388Z","iopub.status.idle":"2025-06-30T03:51:53.782387Z","shell.execute_reply.started":"2025-06-30T03:51:50.134372Z","shell.execute_reply":"2025-06-30T03:51:53.781620Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- DATASET & LOADER ---\nclass TranslationDataset(Dataset):\n    def __init__(self, src, tgt):\n        self.src = torch.LongTensor(src)\n        self.tgt = torch.LongTensor(tgt)\n    def __len__(self):\n        return len(self.src)\n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\nBATCH_SIZE = 64\ntrain_loader = DataLoader(TranslationDataset(train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(TranslationDataset(val_src, val_tgt), batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T03:51:53.783298Z","iopub.execute_input":"2025-06-30T03:51:53.783586Z","iopub.status.idle":"2025-06-30T03:51:53.856115Z","shell.execute_reply.started":"2025-06-30T03:51:53.783565Z","shell.execute_reply":"2025-06-30T03:51:53.855327Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- SCRATCH VANILLA RNN MODULE ---\nclass RNNScratch(nn.Module):\n    def __init__(self, input_size, hidden_size, sigma=0.01):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        def init_weight(*shape):\n            return nn.Parameter(torch.randn(*shape) * sigma)\n        self.W_xh = init_weight(input_size, hidden_size)\n        self.W_hh = init_weight(hidden_size, hidden_size)\n        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n\n    def forward(self, X, H):\n        # X: (batch, input_size), H: (batch, hidden_size)\n        H_next = torch.tanh(X @ self.W_xh + H @ self.W_hh + self.b_h)\n        return H_next\n\n# --- ATTENTION MODULE (unchanged) ---\nclass LuongAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        seq_len, batch_size, hidden = encoder_outputs.size()\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        decoder_hidden = decoder_hidden.unsqueeze(1)\n        energy = torch.bmm(decoder_hidden, self.linear(encoder_outputs).transpose(1, 2))\n        attn_weights = torch.softmax(energy, dim=-1)\n        context = torch.bmm(attn_weights, encoder_outputs)\n        return context.squeeze(1), attn_weights.squeeze(1)\n\n# --- ENCODER WITH VANILLA RNN ---\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = RNNScratch(embed_size, hidden_size)\n\n    def forward(self, x):\n        embeddings = self.embedding(x)\n        H = torch.zeros(x.shape[1], self.rnn.hidden_size, device=x.device)\n        outputs = []\n        for emb in embeddings:\n            H = self.rnn(emb, H)\n            outputs.append(H.unsqueeze(0))\n        return torch.cat(outputs, dim=0), H\n\n# --- DECODER WITH VANILLA RNN & ATTENTION ---\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.attn = LuongAttention(hidden_size)\n        self.rnn = RNNScratch(embed_size + hidden_size, hidden_size)\n        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n\n    def forward(self, x, hidden, encoder_outputs):\n        embeddings = self.embedding(x)\n        outputs = []\n        for emb in embeddings:\n            context, _ = self.attn(hidden, encoder_outputs)\n            rnn_input = torch.cat([emb, context], dim=1)\n            hidden = self.rnn(rnn_input, hidden)\n            out = self.fc(torch.cat([hidden, context], dim=1))\n            outputs.append(out.unsqueeze(0))\n        return torch.cat(outputs, dim=0)\n\n# --- SEQ2SEQ MODULE (unchanged) ---\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, tgt):\n        src = src.transpose(0, 1)\n        tgt = tgt.transpose(0, 1)\n        enc_outputs, hidden = self.encoder(src)\n        outputs = self.decoder(tgt, hidden, enc_outputs)\n        return outputs.transpose(0, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:15:24.426023Z","iopub.execute_input":"2025-06-30T04:15:24.426285Z","iopub.status.idle":"2025-06-30T04:15:24.439002Z","shell.execute_reply.started":"2025-06-30T04:15:24.426269Z","shell.execute_reply":"2025-06-30T04:15:24.438473Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\n\n# --- MODEL INIT ---\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nencoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\ndecoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\nmodel = Seq2Seq(encoder, decoder).to(device)\n\n# --- TRAINING SETUP ---\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\nscaler = GradScaler()\n\n# --- TRAIN LOOP ---\nEPOCHS = 6\nfor epoch in range(EPOCHS):\n    model.train()\n    \n    total_loss = total_tokens = total_correct = 0\n    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        src, tgt = src.to(device), tgt.to(device)\n        # prepare decoder input and target\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        with autocast():\n            logits = model(src, tgt_input)  # (batch, tgt_len-1, vocab)\n            logits = logits.reshape(-1, TGT_VOCAB_SIZE)\n            tgt_flat = tgt_output.reshape(-1)\n            loss = criterion(logits, tgt_flat)\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        # metrics\n        mask = tgt_flat != 0\n        total_loss += loss.item() * mask.sum().item()\n        total_tokens += mask.sum().item()\n        preds = logits.argmax(dim=1)\n        total_correct += (preds == tgt_flat).masked_select(mask).sum().item()\n\n    scheduler.step()\n    print(f\"Epoch {epoch+1} | Loss: {total_loss/total_tokens:.4f} | Acc: {total_correct/total_tokens:.4f}\")\n\n    # validation\n    model.eval()\n    val_loss = val_tokens = val_correct = 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n            logits = model(src, tgt_input).reshape(-1, TGT_VOCAB_SIZE)\n            tgt_flat = tgt_output.reshape(-1)\n            loss = criterion(logits, tgt_flat)\n            mask = tgt_flat != 0\n            val_loss += loss.item() * mask.sum().item()\n            val_tokens += mask.sum().item()\n            preds = logits.argmax(dim=1)\n            val_correct += (preds == tgt_flat).masked_select(mask).sum().item()\n    print(f\"[Val] Loss: {val_loss/val_tokens:.4f} | Acc: {val_correct/val_tokens:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:15:28.525028Z","iopub.execute_input":"2025-06-30T04:15:28.525273Z","iopub.status.idle":"2025-06-30T05:27:44.224629Z","shell.execute_reply.started":"2025-06-30T04:15:28.525257Z","shell.execute_reply":"2025-06-30T05:27:44.224039Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1797441349.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1 Training:   0%|          | 0/1569 [00:00<?, ?it/s]/tmp/ipykernel_35/1797441349.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1 Training: 100%|██████████| 1569/1569 [11:20<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 6.1909 | Acc: 0.1496\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 Validation: 100%|██████████| 197/197 [00:44<00:00,  4.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 5.5365 | Acc: 0.2008\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|██████████| 1569/1569 [11:20<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Loss: 5.1217 | Acc: 0.2589\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Validation: 100%|██████████| 197/197 [00:44<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 4.8460 | Acc: 0.3264\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|██████████| 1569/1569 [11:16<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Loss: 3.7188 | Acc: 0.4635\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Validation: 100%|██████████| 197/197 [00:44<00:00,  4.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 3.1528 | Acc: 0.5313\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|██████████| 1569/1569 [11:16<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Loss: 2.1520 | Acc: 0.6467\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Validation: 100%|██████████| 197/197 [00:44<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 2.1920 | Acc: 0.6481\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Training: 100%|██████████| 1569/1569 [11:15<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Loss: 1.5795 | Acc: 0.7189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Validation: 100%|██████████| 197/197 [00:44<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 1.7153 | Acc: 0.7063\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Training: 100%|██████████| 1569/1569 [11:16<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Loss: 1.3691 | Acc: 0.7460\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Validation: 100%|██████████| 197/197 [00:44<00:00,  4.41it/s]","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 1.7153 | Acc: 0.7063\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- SAVE MODEL & TOKENIZERS ---\nimport pickle\n\ntorch.save(model.state_dict(), \"rnn_seq2seq.pt\")\nwith open(\"src_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(src_tokenizer, f)\nwith open(\"tgt_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tgt_tokenizer, f)\n\nprint(\"Training complete, model and tokenizers saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:15:10.474220Z","iopub.status.idle":"2025-06-30T04:15:10.474448Z","shell.execute_reply.started":"2025-06-30T04:15:10.474340Z","shell.execute_reply":"2025-06-30T04:15:10.474350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- GREEDY DECODE for Vanilla RNN ---\nidx2word = {idx: word for word, idx in tgt_tokenizer.word_index.items()}\nidx2word[0] = '<pad>'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:29:19.393525Z","iopub.execute_input":"2025-06-30T05:29:19.394331Z","iopub.status.idle":"2025-06-30T05:29:19.401281Z","shell.execute_reply.started":"2025-06-30T05:29:19.394308Z","shell.execute_reply":"2025-06-30T05:29:19.400675Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def greedy_decode(model, sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=70):\n    model.eval()\n    cleaned = clean_text(sentence)\n    seq = encode_and_pad([cleaned], src_tokenizer)\n    src_tensor = torch.LongTensor(seq).to(device).transpose(0, 1)\n\n    start_id = tgt_tokenizer.word_index['<start>']\n    end_id = tgt_tokenizer.word_index['<end>']\n    tgt_ids = [start_id]\n    result = []\n\n    with torch.no_grad():\n        encoder_outputs, hidden = model.encoder(src_tensor)\n        for _ in range(max_len):\n            tgt_tensor = torch.LongTensor([tgt_ids]).to(device).transpose(0, 1)\n            logits = model.decoder(tgt_tensor, hidden, encoder_outputs)\n            next_id = logits[-1, 0].argmax().item()\n            if next_id == end_id:\n                break\n            result.append(idx2word.get(next_id, '<unk>'))\n            tgt_ids.append(next_id)\n\n    return ' '.join(result)\n\n# --- TEST ---\ntest_sentences = [\n    \"toi yeu tieng viet\",\n    \"chung ta se chien thang\",\n    \"ha noi la thu do cua viet nam\"\n]\nfor sent in test_sentences:\n    print(f\"Input: {sent}\")\n    print(f\"Output: {greedy_decode(model, sent, src_tokenizer, tgt_tokenizer, idx2word)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:30:50.958911Z","iopub.execute_input":"2025-06-30T05:30:50.959446Z","iopub.status.idle":"2025-06-30T05:30:51.081085Z","shell.execute_reply.started":"2025-06-30T05:30:50.959426Z","shell.execute_reply":"2025-06-30T05:30:51.080338Z"}},"outputs":[{"name":"stdout","text":"Input: toi yeu tieng viet\nOutput: tôi yêu tiếng việt\nInput: chung ta se chien thang\nOutput: chứng ta sẽ chiến thắng\nInput: ha noi la thu do cua viet nam\nOutput: hà nội là thủ đô của việt nam\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def beam_search_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, beam_width=3, max_len=70):\n    model.eval()\n\n    # --- Clean and encode input ---\n    cleaned_input = input_sentence.strip().lower()\n    input_seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned_input]), maxlen=max_len, padding='post')\n    input_tensor = torch.LongTensor(input_seq).to(device)\n\n    start_token = tgt_tokenizer.word_index.get('<start>', 1)\n    end_token = tgt_tokenizer.word_index.get('<end>', 2)\n\n    sequences = [[start_token]]\n    scores = [0.0]\n    completed_sequences = []\n\n    for _ in range(max_len):\n        all_candidates = []\n        for seq, score in zip(sequences, scores):\n            if seq[-1] == end_token:\n                completed_sequences.append((seq, score))\n                continue\n\n            tgt_tensor = torch.LongTensor([seq]).to(device)\n            with torch.no_grad():\n                output = model(input_tensor, tgt_tensor)\n                logits = output[0, -1, :]\n                log_probs = torch.log_softmax(logits, dim=-1)\n\n            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n            for j in range(beam_width):\n                next_token = topk_indices[j].item()\n                next_score = score + topk_log_probs[j].item()\n                all_candidates.append((seq + [next_token], next_score))\n\n        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n        sequences = [seq for seq, _ in ordered[:beam_width]]\n        scores = [score for _, score in ordered[:beam_width]]\n\n        if all(seq[-1] == end_token for seq in sequences):\n            break\n\n    if completed_sequences:\n        best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n    else:\n        best_seq = sequences[0]\n\n    decoded = []\n    for token in best_seq[1:]:\n        if token == end_token:\n            break\n        decoded.append(idx2word.get(token, '<unk>'))\n\n    return ' '.join(decoded)\n\n# --- Example test ---\ntest_sentences = [\n    \"toi yeu tieng viet\",\n    \"chung ta se chien thang\",\n    \"ha noi la thu do cua viet nam\"\n]\n\nprint(\"\\nKết quả dự đoán:\")\nfor sent in test_sentences:\n    print(\"Input:\", sent)\n    print(\"Output:\", beam_search_decode(model, sent, src_tokenizer, tgt_tokenizer, {v:k for k,v in tgt_tokenizer.word_index.items()}, beam_width=5))\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:29:23.079700Z","iopub.execute_input":"2025-06-30T05:29:23.080365Z","iopub.status.idle":"2025-06-30T05:29:24.433304Z","shell.execute_reply.started":"2025-06-30T05:29:23.080342Z","shell.execute_reply":"2025-06-30T05:29:24.432651Z"}},"outputs":[{"name":"stdout","text":"\nKết quả dự đoán:\nInput: toi yeu tieng viet\nOutput: tôi yêu tiếng việt\n\nInput: chung ta se chien thang\nOutput: chứng ta sẽ chiến thắng\n\nInput: ha noi la thu do cua viet nam\nOutput: hà nội là thủ đô của việt nam\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:30:59.288452Z","iopub.execute_input":"2025-06-30T05:30:59.288720Z","iopub.status.idle":"2025-06-30T05:31:03.755653Z","shell.execute_reply.started":"2025-06-30T05:30:59.288701Z","shell.execute_reply":"2025-06-30T05:31:03.754949Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# --- Advanced BLEU Evaluation Function (Optimized and Precise) ---\nimport sacrebleu\nfrom tqdm import tqdm\nimport torch\n\ndef evaluate_bleu(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n    model.eval()\n    references = []\n    hypotheses = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating BLEU\"):\n        input_sentence = row['no_diacritics_clean']\n        reference = row['with_diacritics_clean']\n\n        # Clean target (remove <start> and <end>)\n        reference = reference.replace('<start>', '').replace('<end>', '').strip()\n\n        # Decode prediction from model\n        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n        prediction = prediction.strip()\n\n        references.append([reference])\n        hypotheses.append(prediction)\n\n    # Compute BLEU\n    bleu = sacrebleu.corpus_bleu(hypotheses, list(map(list, zip(*references))))\n    print(f\"\\nFinal BLEU Score: {bleu.score:.2f}\")\n    return bleu.score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:31:06.738127Z","iopub.execute_input":"2025-06-30T05:31:06.738424Z","iopub.status.idle":"2025-06-30T05:31:06.821993Z","shell.execute_reply.started":"2025-06-30T05:31:06.738397Z","shell.execute_reply":"2025-06-30T05:31:06.821270Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"score = evaluate_bleu(model, df_test.sample(1000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:31:35.283847Z","iopub.execute_input":"2025-06-30T05:31:35.284126Z","iopub.status.idle":"2025-06-30T05:34:38.006302Z","shell.execute_reply.started":"2025-06-30T05:31:35.284108Z","shell.execute_reply":"2025-06-30T05:34:38.005650Z"}},"outputs":[{"name":"stderr","text":"Evaluating BLEU: 100%|██████████| 1000/1000 [03:02<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFinal BLEU Score: 46.06\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"score = evaluate_bleu(model, df_test.sample(1000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:34:38.007329Z","iopub.execute_input":"2025-06-30T05:34:38.007578Z","iopub.status.idle":"2025-06-30T05:54:08.004705Z","shell.execute_reply.started":"2025-06-30T05:34:38.007560Z","shell.execute_reply":"2025-06-30T05:54:08.004025Z"}},"outputs":[{"name":"stderr","text":"Evaluating BLEU: 100%|██████████| 1000/1000 [19:29<00:00,  1.17s/it] ","output_type":"stream"},{"name":"stdout","text":"\nFinal BLEU Score: 49.67\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- Advanced ChrF++ Evaluation Function ---\nimport sacrebleu\nfrom tqdm import tqdm\n\ndef evaluate_chrf(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n    model.eval()\n    references = []\n    hypotheses = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating ChrF++\"):\n        input_sentence = row['no_diacritics_clean']\n        reference = row['with_diacritics_clean'].replace('<start>', '').replace('<end>', '').strip()\n\n        # Decode prediction using the provided decode_fn\n        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n        prediction = prediction.strip()  # already stripped from <start>/<end> if decode_fn is correct\n\n        # Append for evaluation\n        references.append([reference])  # list of references for each sentence\n        hypotheses.append(prediction)   # single prediction per sentence\n\n    # Compute ChrF++ score\n    chrf = sacrebleu.corpus_chrf(hypotheses, list(map(list, zip(*references))))\n    print(f\"\\nFinal ChrF++ Score: {chrf.score:.2f}\")\n    return chrf.score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:54:08.005968Z","iopub.execute_input":"2025-06-30T05:54:08.006177Z","iopub.status.idle":"2025-06-30T05:54:08.011834Z","shell.execute_reply.started":"2025-06-30T05:54:08.006162Z","shell.execute_reply":"2025-06-30T05:54:08.011212Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"chrf_score = evaluate_chrf(model, df_test.sample(1000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T05:54:08.012599Z","iopub.execute_input":"2025-06-30T05:54:08.012870Z","iopub.status.idle":"2025-06-30T05:56:56.825196Z","shell.execute_reply.started":"2025-06-30T05:54:08.012848Z","shell.execute_reply":"2025-06-30T05:56:56.824431Z"}},"outputs":[{"name":"stderr","text":"Evaluating ChrF++: 100%|██████████| 1000/1000 [02:48<00:00,  5.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFinal ChrF++ Score: 66.50\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"chrf_score = evaluate_chrf(model, df_test.sample(10), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:23:36.576387Z","iopub.execute_input":"2025-06-30T06:23:36.576648Z","iopub.status.idle":"2025-06-30T06:23:55.653022Z","shell.execute_reply.started":"2025-06-30T06:23:36.576629Z","shell.execute_reply":"2025-06-30T06:23:55.652351Z"}},"outputs":[{"name":"stderr","text":"Evaluating ChrF++: 100%|██████████| 10/10 [00:19<00:00,  1.91s/it]","output_type":"stream"},{"name":"stdout","text":"\nFinal ChrF++ Score: 68.52\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27}]}