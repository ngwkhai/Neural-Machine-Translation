{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12201813,"sourceType":"datasetVersion","datasetId":7685807}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport re\nimport string\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- DEVICE SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- DATA CLEANING ---\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# --- LOAD & PREPROCESS ---\npath = \"/kaggle/input/vietnamese-diacritics-dataset/\"\ndf_train = pd.read_csv(path + \"ViDiacritics_train.csv\")\ndf_val   = pd.read_csv(path + \"ViDiacritics_val.csv\")\ndf_test  = pd.read_csv(path + \"ViDiacritics_test.csv\")\n\n# sample for speed\ndf_train = df_train.sample(frac=0.05, random_state=42).reset_index(drop=True)\ndf_val   = df_val.sample(frac=0.05, random_state=42).reset_index(drop=True)\ndf_test  = df_test.sample(frac=0.05, random_state=42).reset_index(drop=True)\n\n# apply cleaning and add tokens\nfor df in [df_train, df_val, df_test]:\n    df['no_diacritics_clean']   = df['no_diacritics'].astype(str).apply(clean_text)\n    df['with_diacritics_clean'] = df['with_diacritics'].astype(str).apply(clean_text)\n    df['with_diacritics_clean'] = df['with_diacritics_clean'].apply(lambda x: '<start> ' + x + ' <end>')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- TOKENIZATION ---\nfilters = '\"!#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # preserve <> for start/end\nsrc_tokenizer = Tokenizer(oov_token='<unk>', filters=filters)\ntgt_tokenizer = Tokenizer(oov_token='<unk>', filters=filters)\nsrc_tokenizer.fit_on_texts(df_train['no_diacritics_clean'])\ntgt_tokenizer.fit_on_texts(df_train['with_diacritics_clean'])\n\nSRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\nTGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\nMAX_LEN = 70","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encode & pad\ndef encode_and_pad(texts, tokenizer):\n    seqs = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post')\n\ntrain_src = encode_and_pad(df_train['no_diacritics_clean'], src_tokenizer)\ntrain_tgt = encode_and_pad(df_train['with_diacritics_clean'], tgt_tokenizer)\nval_src   = encode_and_pad(df_val['no_diacritics_clean'], src_tokenizer)\nval_tgt   = encode_and_pad(df_val['with_diacritics_clean'], tgt_tokenizer)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- DATASET & LOADER ---\nclass TranslationDataset(Dataset):\n    def __init__(self, src, tgt):\n        self.src = torch.LongTensor(src)\n        self.tgt = torch.LongTensor(tgt)\n    def __len__(self):\n        return len(self.src)\n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\nBATCH_SIZE = 64\ntrain_loader = DataLoader(TranslationDataset(train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(TranslationDataset(val_src, val_tgt), batch_size=BATCH_SIZE)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- SCRATCH LSTM MODULE ---\nclass LSTMScratch(nn.Module):\n    def __init__(self, input_size, hidden_size, sigma=0.01):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        def init_weight(*shape):\n            return nn.Parameter(torch.randn(*shape) * sigma)\n\n        def init_gate():\n            return (\n                init_weight(input_size, hidden_size),\n                init_weight(hidden_size, hidden_size),\n                nn.Parameter(torch.zeros(hidden_size))\n            )\n\n        # Input gate\n        self.W_xi, self.W_hi, self.b_i = init_gate()\n        # Forget gate\n        self.W_xf, self.W_hf, self.b_f = init_gate()\n        # Output gate\n        self.W_xo, self.W_ho, self.b_o = init_gate()\n        # Candidate cell state\n        self.W_xc, self.W_hc, self.b_c = init_gate()\n\n    def forward(self, X, H, C):\n        I = torch.sigmoid(X @ self.W_xi + H @ self.W_hi + self.b_i)\n        F = torch.sigmoid(X @ self.W_xf + H @ self.W_hf + self.b_f)\n        O = torch.sigmoid(X @ self.W_xo + H @ self.W_ho + self.b_o)\n        C_tilde = torch.tanh(X @ self.W_xc + H @ self.W_hc + self.b_c)\n        C_next = F * C + I * C_tilde\n        H_next = O * torch.tanh(C_next)\n        return H_next, C_next\n\n# --- ATTENTION MODULE ---\nclass LuongAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: (batch, hidden)\n        # encoder_outputs: (seq_len, batch, hidden)\n        seq_len, batch_size, hidden = encoder_outputs.size()\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # (batch, seq_len, hidden)\n        decoder_hidden = decoder_hidden.unsqueeze(1)        # (batch, 1, hidden)\n        # Score = h_t^T W h_s\n        energy = torch.bmm(decoder_hidden, self.linear(encoder_outputs).transpose(1, 2))  # (batch, 1, seq_len)\n        attn_weights = torch.softmax(energy, dim=-1)  # (batch, 1, seq_len)\n        context = torch.bmm(attn_weights, encoder_outputs)  # (batch, 1, hidden)\n        return context.squeeze(1), attn_weights.squeeze(1)  # (batch, hidden), (batch, seq_len)\n\n# --- ENCODER WITH LSTM ---\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = LSTMScratch(embed_size, hidden_size)\n\n    def forward(self, x):\n        embeddings = self.embedding(x)\n        H = torch.zeros(x.shape[1], self.lstm.hidden_size, device=x.device)\n        C = torch.zeros_like(H)\n        outputs = []\n        for emb in embeddings:\n            H, C = self.lstm(emb, H, C)\n            outputs.append(H.unsqueeze(0))\n        return torch.cat(outputs, dim=0), (H, C)\n\n# --- DECODER WITH LSTM AND ATTENTION ---\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.attn = LuongAttention(hidden_size)\n        self.lstm = LSTMScratch(embed_size + hidden_size, hidden_size)\n        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n\n    def forward(self, x, state, encoder_outputs):\n        H, C = state\n        embeddings = self.embedding(x)\n        outputs = []\n        for emb in embeddings:\n            context, _ = self.attn(H, encoder_outputs)\n            lstm_input = torch.cat([emb, context], dim=1)\n            H, C = self.lstm(lstm_input, H, C)\n            out = self.fc(torch.cat([H, context], dim=1))\n            outputs.append(out.unsqueeze(0))\n        return torch.cat(outputs, dim=0), (H, C)\n\n# --- SEQ2SEQ ---\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, tgt):\n        src = src.transpose(0, 1)\n        tgt = tgt.transpose(0, 1)\n        enc_outputs, (hidden, cell) = self.encoder(src)\n        outputs, _ = self.decoder(tgt, (hidden, cell), enc_outputs)\n        return outputs.transpose(0, 1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n# --- MODEL INIT ---\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nencoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\ndecoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE).to(device)\nmodel = Seq2Seq(encoder, decoder).to(device)\n\n# --- TRAINING SETUP ---\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\nscaler = GradScaler()\n\n# --- TRAIN LOOP ---\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    model.train()\n    \n    total_loss = total_tokens = total_correct = 0\n    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        src, tgt = src.to(device), tgt.to(device)\n        # prepare decoder input and target\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        with autocast():\n            logits = model(src, tgt_input)  # (batch, tgt_len-1, vocab)\n            logits = logits.reshape(-1, TGT_VOCAB_SIZE)\n            tgt_flat = tgt_output.reshape(-1)\n            loss = criterion(logits, tgt_flat)\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        # metrics\n        mask = tgt_flat != 0\n        total_loss += loss.item() * mask.sum().item()\n        total_tokens += mask.sum().item()\n        preds = logits.argmax(dim=1)\n        total_correct += (preds == tgt_flat).masked_select(mask).sum().item()\n\n    scheduler.step()\n    print(f\"Epoch {epoch+1} | Loss: {total_loss/total_tokens:.4f} | Acc: {total_correct/total_tokens:.4f}\")\n\n    # validation\n    model.eval()\n    val_loss = val_tokens = val_correct = 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n            logits = model(src, tgt_input).reshape(-1, TGT_VOCAB_SIZE)\n            tgt_flat = tgt_output.reshape(-1)\n            loss = criterion(logits, tgt_flat)\n            mask = tgt_flat != 0\n            val_loss += loss.item() * mask.sum().item()\n            val_tokens += mask.sum().item()\n            preds = logits.argmax(dim=1)\n            val_correct += (preds == tgt_flat).masked_select(mask).sum().item()\n    print(f\"[Val] Loss: {val_loss/val_tokens:.4f} | Acc: {val_correct/val_tokens:.4f}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- SAVE MODEL & TOKENIZERS ---\nimport pickle\n\ntorch.save(model.state_dict(), \"lstm_seq2seq.pt\")\nwith open(\"src_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(src_tokenizer, f)\nwith open(\"tgt_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tgt_tokenizer, f)\n\nprint(\"Training complete, model and tokenizers saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- GREEDY DECODE (LSTM-Compatible with Attention) ---\nidx2word = {idx: word for word, idx in tgt_tokenizer.word_index.items()}\nidx2word[0] = '<pad>'\n\ndef greedy_decode(model, sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=70):\n    model.eval()\n    cleaned = clean_text(sentence)\n    seq = encode_and_pad([cleaned], src_tokenizer)\n    src_tensor = torch.LongTensor(seq).to(device).transpose(0, 1)\n\n    start_id = tgt_tokenizer.word_index['<start>']\n    end_id = tgt_tokenizer.word_index['<end>']\n    tgt_ids = [start_id]\n    result = []\n\n    with torch.no_grad():\n        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n        for _ in range(max_len):\n            tgt_tensor = torch.LongTensor([tgt_ids[-1]]).unsqueeze(0).to(device)  # (1, 1)\n            logits, (hidden, cell) = model.decoder(tgt_tensor, (hidden, cell), encoder_outputs)\n            next_id = logits[-1, 0].argmax().item()\n            if next_id == end_id:\n                break\n            result.append(idx2word.get(next_id, '<unk>'))\n            tgt_ids.append(next_id)\n\n    return ' '.join(result)\n\n# --- TEST ---\ntest_sentences = [\n    \"toi yeu tieng viet\",\n    \"chung ta se chien thang\",\n    \"ha noi la thu do cua viet nam\"\n]\nfor sent in test_sentences:\n    print(f\"Input: {sent}\")\n    print(f\"Output: {greedy_decode(model, sent, src_tokenizer, tgt_tokenizer, idx2word)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def beam_search_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, beam_width=3, max_len=70):\n    model.eval()\n\n    # --- Clean and encode input ---\n    cleaned_input = input_sentence.strip().lower()\n    input_seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned_input]), maxlen=max_len, padding='post')\n    input_tensor = torch.LongTensor(input_seq).to(device)\n\n    start_token = tgt_tokenizer.word_index.get('<start>', 1)\n    end_token = tgt_tokenizer.word_index.get('<end>', 2)\n\n    sequences = [[start_token]]\n    scores = [0.0]\n    completed_sequences = []\n\n    for _ in range(max_len):\n        all_candidates = []\n        for seq, score in zip(sequences, scores):\n            if seq[-1] == end_token:\n                completed_sequences.append((seq, score))\n                continue\n\n            tgt_tensor = torch.LongTensor([seq]).to(device)\n            with torch.no_grad():\n                output = model(input_tensor, tgt_tensor)\n                logits = output[0, -1, :]\n                log_probs = torch.log_softmax(logits, dim=-1)\n\n            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n            for j in range(beam_width):\n                next_token = topk_indices[j].item()\n                next_score = score + topk_log_probs[j].item()\n                all_candidates.append((seq + [next_token], next_score))\n\n        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n        sequences = [seq for seq, _ in ordered[:beam_width]]\n        scores = [score for _, score in ordered[:beam_width]]\n\n        if all(seq[-1] == end_token for seq in sequences):\n            break\n\n    if completed_sequences:\n        best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n    else:\n        best_seq = sequences[0]\n\n    decoded = []\n    for token in best_seq[1:]:\n        if token == end_token:\n            break\n        decoded.append(idx2word.get(token, '<unk>'))\n\n    return ' '.join(decoded)\n\n# --- Example test ---\ntest_sentences = [\n    \"toi yeu tieng viet\",\n    \"chung ta se chien thang\",\n    \"ha noi la thu do cua viet nam\"\n]\n\nprint(\"\\nKết quả dự đoán:\")\nfor sent in test_sentences:\n    print(\"Input:\", sent)\n    print(\"Output:\", beam_search_decode(model, sent, src_tokenizer, tgt_tokenizer, {v:k for k,v in tgt_tokenizer.word_index.items()}, beam_width=5))\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Advanced BLEU Evaluation Function (Optimized and Precise) ---\nimport sacrebleu\nfrom tqdm import tqdm\nimport torch\n\ndef evaluate_bleu(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n    model.eval()\n    references = []\n    hypotheses = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating BLEU\"):\n        input_sentence = row['no_diacritics_clean']\n        reference = row['with_diacritics_clean']\n\n        # Clean target (remove <start> and <end>)\n        reference = reference.replace('<start>', '').replace('<end>', '').strip()\n\n        # Decode prediction from model\n        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n        prediction = prediction.strip()\n\n        references.append([reference])\n        hypotheses.append(prediction)\n\n    # Compute BLEU\n    bleu = sacrebleu.corpus_bleu(hypotheses, list(map(list, zip(*references))))\n    print(f\"\\nFinal BLEU Score: {bleu.score:.2f}\")\n    return bleu.score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score = evaluate_bleu(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score = evaluate_bleu(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Advanced ChrF++ Evaluation Function ---\nimport sacrebleu\nfrom tqdm import tqdm\n\ndef evaluate_chrf(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n    model.eval()\n    references = []\n    hypotheses = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating ChrF++\"):\n        input_sentence = row['no_diacritics_clean']\n        reference = row['with_diacritics_clean'].replace('<start>', '').replace('<end>', '').strip()\n\n        # Decode prediction using the provided decode_fn\n        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n        prediction = prediction.strip()  # already stripped from <start>/<end> if decode_fn is correct\n\n        # Append for evaluation\n        references.append([reference])  # list of references for each sentence\n        hypotheses.append(prediction)   # single prediction per sentence\n\n    # Compute ChrF++ score\n    chrf = sacrebleu.corpus_chrf(hypotheses, list(map(list, zip(*references))))\n    print(f\"\\nFinal ChrF++ Score: {chrf.score:.2f}\")\n    return chrf.score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chrf_score = evaluate_chrf(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chrf_score = evaluate_chrf(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}