{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6d249b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:40:30.749559Z",
     "iopub.status.busy": "2025-06-28T10:40:30.749279Z",
     "iopub.status.idle": "2025-06-28T10:41:01.929213Z",
     "shell.execute_reply": "2025-06-28T10:41:01.928296Z"
    },
    "papermill": {
     "duration": 31.186605,
     "end_time": "2025-06-28T10:41:01.930901",
     "exception": false,
     "start_time": "2025-06-28T10:40:30.744296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 10:40:43.178786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751107243.584917      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751107243.691809      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f368fa0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:41:01.940041Z",
     "iopub.status.busy": "2025-06-28T10:41:01.938999Z",
     "iopub.status.idle": "2025-06-28T10:41:02.039755Z",
     "shell.execute_reply": "2025-06-28T10:41:02.039133Z"
    },
    "papermill": {
     "duration": 0.106399,
     "end_time": "2025-06-28T10:41:02.041182",
     "exception": false,
     "start_time": "2025-06-28T10:41:01.934783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- DEVICE SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c546e6bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:41:02.048857Z",
     "iopub.status.busy": "2025-06-28T10:41:02.048404Z",
     "iopub.status.idle": "2025-06-28T10:42:24.893384Z",
     "shell.execute_reply": "2025-06-28T10:42:24.892488Z"
    },
    "papermill": {
     "duration": 82.850369,
     "end_time": "2025-06-28T10:42:24.894973",
     "exception": false,
     "start_time": "2025-06-28T10:41:02.044604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- LOAD DATA ---\n",
    "path = \"/kaggle/input/vietnamese-diacritics-dataset/\"\n",
    "df_train = pd.read_csv(path + \"ViDiacritics_train.csv\")\n",
    "df_val   = pd.read_csv(path + \"ViDiacritics_val.csv\")\n",
    "df_test  = pd.read_csv(path + \"ViDiacritics_test.csv\")\n",
    "\n",
    "df_train = df_train.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "df_val   = df_val.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "df_test  = df_test.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372f25a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:42:24.907240Z",
     "iopub.status.busy": "2025-06-28T10:42:24.906565Z",
     "iopub.status.idle": "2025-06-28T10:42:54.684038Z",
     "shell.execute_reply": "2025-06-28T10:42:54.683140Z"
    },
    "papermill": {
     "duration": 29.787004,
     "end_time": "2025-06-28T10:42:54.685679",
     "exception": false,
     "start_time": "2025-06-28T10:42:24.898675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CLEAN TEXT ---\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def apply_cleaning(df):\n",
    "    df['no_diacritics_clean']   = df['no_diacritics'].astype(str).apply(clean_text)\n",
    "    df['with_diacritics_clean'] = df['with_diacritics'].astype(str).apply(clean_text)\n",
    "    df['with_diacritics_clean'] = df['with_diacritics_clean'].apply(lambda x: '<start> ' + x + ' <end>')\n",
    "    return df\n",
    "\n",
    "df_train = apply_cleaning(df_train)\n",
    "df_val   = apply_cleaning(df_val)\n",
    "df_test  = apply_cleaning(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5317de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:42:54.694299Z",
     "iopub.status.busy": "2025-06-28T10:42:54.693672Z",
     "iopub.status.idle": "2025-06-28T10:43:32.619831Z",
     "shell.execute_reply": "2025-06-28T10:43:32.619005Z"
    },
    "papermill": {
     "duration": 37.931782,
     "end_time": "2025-06-28T10:43:32.621461",
     "exception": false,
     "start_time": "2025-06-28T10:42:54.689679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- TOKENIZATION ---\n",
    "tokenizer_filters = '\"!#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # giữ <>\n",
    "src_tokenizer = Tokenizer(oov_token='<unk>', filters=tokenizer_filters)\n",
    "tgt_tokenizer = Tokenizer(oov_token='<unk>', filters=tokenizer_filters)\n",
    "src_tokenizer.fit_on_texts(df_train['no_diacritics_clean'])\n",
    "tgt_tokenizer.fit_on_texts(df_train['with_diacritics_clean'])\n",
    "\n",
    "SRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\n",
    "TGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\n",
    "MAX_LEN = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02a5e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:43:32.629238Z",
     "iopub.status.busy": "2025-06-28T10:43:32.629011Z",
     "iopub.status.idle": "2025-06-28T10:44:13.595832Z",
     "shell.execute_reply": "2025-06-28T10:44:13.595240Z"
    },
    "papermill": {
     "duration": 40.972209,
     "end_time": "2025-06-28T10:44:13.597284",
     "exception": false,
     "start_time": "2025-06-28T10:43:32.625075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ENCODING ---\n",
    "def encode_and_pad(texts, tokenizer):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "train_src = encode_and_pad(df_train['no_diacritics_clean'], src_tokenizer)\n",
    "train_tgt = encode_and_pad(df_train['with_diacritics_clean'], tgt_tokenizer)\n",
    "val_src   = encode_and_pad(df_val['no_diacritics_clean'], src_tokenizer)\n",
    "val_tgt   = encode_and_pad(df_val['with_diacritics_clean'], tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ca6add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:44:13.605523Z",
     "iopub.status.busy": "2025-06-28T10:44:13.605030Z",
     "iopub.status.idle": "2025-06-28T10:44:14.189379Z",
     "shell.execute_reply": "2025-06-28T10:44:14.188662Z"
    },
    "papermill": {
     "duration": 0.589889,
     "end_time": "2025-06-28T10:44:14.190882",
     "exception": false,
     "start_time": "2025-06-28T10:44:13.600993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- DATASET ---\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = torch.LongTensor(src)\n",
    "        self.tgt = torch.LongTensor(tgt)\n",
    "    def __len__(self): return len(self.src)\n",
    "    def __getitem__(self, idx): return self.src[idx], self.tgt[idx]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(TranslationDataset(train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TranslationDataset(val_src, val_tgt), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce19e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:44:14.199453Z",
     "iopub.status.busy": "2025-06-28T10:44:14.199049Z",
     "iopub.status.idle": "2025-06-28T10:44:14.219414Z",
     "shell.execute_reply": "2025-06-28T10:44:14.218841Z"
    },
    "papermill": {
     "duration": 0.025793,
     "end_time": "2025-06-28T10:44:14.220454",
     "exception": false,
     "start_time": "2025-06-28T10:44:14.194661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- TRANSFORMER MODULES ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        Q, K, V = Q.float(), K.float(), V.float()\n",
    "        scores = Q.matmul(K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        return attn.matmul(V)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        b, seq_len, _ = x.size()\n",
    "        return x.view(b, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        b, h, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(b, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        out = self.scaled_dot_product(Q, K, V, mask)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.W_o(out)\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff   = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1= nn.LayerNorm(d_model)\n",
    "        self.norm2= nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.norm1(x + self.dropout(self.attn(x,x,x,mask)))\n",
    "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn  = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff         = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1      = nn.LayerNorm(d_model)\n",
    "        self.norm2      = nn.LayerNorm(d_model)\n",
    "        self.norm3      = nn.LayerNorm(d_model)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x,x,x,tgt_mask)))\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(x,enc_out,enc_out,src_mask)))\n",
    "        x = self.norm3(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8, num_layers=2, d_ff=512, max_len=70, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        seq_len = tgt.size(1)\n",
    "        nopeak = torch.tril(torch.ones(1,1,seq_len,seq_len, device=tgt.device)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        x = self.dropout(self.pos_enc(self.enc_emb(src)))\n",
    "        y = self.dropout(self.pos_enc(self.dec_emb(tgt)))\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        for layer in self.dec_layers:\n",
    "            y = layer(y, x, src_mask, tgt_mask)\n",
    "        return self.fc_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "095fbcae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:44:14.227769Z",
     "iopub.status.busy": "2025-06-28T10:44:14.227563Z",
     "iopub.status.idle": "2025-06-28T14:55:22.142210Z",
     "shell.execute_reply": "2025-06-28T14:55:22.141289Z"
    },
    "papermill": {
     "duration": 15067.920178,
     "end_time": "2025-06-28T14:55:22.143829",
     "exception": false,
     "start_time": "2025-06-28T10:44:14.223651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1068574685.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "Epoch 1 Training:   0%|          | 0/15688 [00:00<?, ?it/s]/tmp/ipykernel_19/1068574685.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Training: 100%|██████████| 15688/15688 [46:09<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.1965 | Acc: 0.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 1961/1961 [04:01<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.5842 | Acc: 0.8869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 15688/15688 [46:16<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.6247 | Acc: 0.8731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|██████████| 1961/1961 [04:02<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.4819 | Acc: 0.9063\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 15688/15688 [46:04<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.5220 | Acc: 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|██████████| 1961/1961 [04:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.4305 | Acc: 0.9161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 15688/15688 [46:04<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.4614 | Acc: 0.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|██████████| 1961/1961 [04:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.3919 | Acc: 0.9239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 15688/15688 [46:15<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.4276 | Acc: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|██████████| 1961/1961 [04:01<00:00,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.3758 | Acc: 0.9272\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL & TRAINING ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "model     = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "scaler    = GradScaler()\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, total_tokens, total_correct = 0, 0, 0\n",
    "    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits = model(src, tgt_input).view(-1, TGT_VOCAB_SIZE)\n",
    "            tgt_flat = tgt_output.reshape(-1)\n",
    "            loss = criterion(logits, tgt_flat)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        mask = tgt_flat != 0\n",
    "        total_loss   += loss.item() * mask.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct+= (preds == tgt_flat).masked_select(mask).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = total_loss / total_tokens\n",
    "    train_acc  = total_correct / total_tokens\n",
    "    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_tokens, val_correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            logits = model(src, tgt_input).view(-1, TGT_VOCAB_SIZE)\n",
    "            tgt_flat = tgt_output.reshape(-1)\n",
    "            loss = criterion(logits, tgt_flat)\n",
    "            mask = tgt_flat != 0\n",
    "            val_loss   += loss.item() * mask.sum().item()\n",
    "            val_tokens += mask.sum().item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct+= (preds == tgt_flat).masked_select(mask).sum().item()\n",
    "    print(f\"[Val] Loss: {val_loss/val_tokens:.4f} | Acc: {val_correct/val_tokens:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703bba16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:30.344438Z",
     "iopub.status.busy": "2025-06-28T14:55:30.343511Z",
     "iopub.status.idle": "2025-06-28T14:55:31.982552Z",
     "shell.execute_reply": "2025-06-28T14:55:31.981583Z"
    },
    "papermill": {
     "duration": 5.896204,
     "end_time": "2025-06-28T14:55:31.983926",
     "exception": false,
     "start_time": "2025-06-28T14:55:26.087722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_model.pt\n",
      "Model package zipped to transformer_model_package.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "# Tên file lưu model\n",
    "model_filename = \"transformer_model.pt\"\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# Lưu tokenizer (dùng pickle hoặc joblib)\n",
    "import pickle\n",
    "with open(\"src_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(src_tokenizer, f)\n",
    "with open(\"tgt_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tgt_tokenizer, f)\n",
    "\n",
    "# Tạo danh sách file cần zip\n",
    "files_to_zip = [\n",
    "    \"transformer_model.pt\",\n",
    "    \"src_tokenizer.pkl\",\n",
    "    \"tgt_tokenizer.pkl\"\n",
    "]\n",
    "\n",
    "# Nén thành file zip\n",
    "zip_filename = \"transformer_model_package.zip\"\n",
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    for file in files_to_zip:\n",
    "        if os.path.exists(file):\n",
    "            zipf.write(file)\n",
    "\n",
    "print(f\"Model package zipped to {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a5f335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:40.071135Z",
     "iopub.status.busy": "2025-06-28T14:55:40.070316Z",
     "iopub.status.idle": "2025-06-28T14:55:40.626667Z",
     "shell.execute_reply": "2025-06-28T14:55:40.625657Z"
    },
    "papermill": {
     "duration": 4.696125,
     "end_time": "2025-06-28T14:55:40.627915",
     "exception": false,
     "start_time": "2025-06-28T14:55:35.931790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả dự đoán:\n",
      "Input: toi yeu tieng viet\n",
      "Output: tôi yêu tiếng việt\n",
      "\n",
      "Input: chung ta se chien thang\n",
      "Output: chúng ta sẽ chiến thắng\n",
      "\n",
      "Input: ha noi la thu do cua viet nam\n",
      "Output: hà nội là thủ đô của việt nam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UTILITY FUNCTIONS ---\n",
    "idx2word = {idx:word for word,idx in tgt_tokenizer.word_index.items()}\n",
    "idx2word[0] = '<pad>'\n",
    "\n",
    "def greedy_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=70):\n",
    "    model.eval()\n",
    "    cleaned = clean_text(input_sentence)\n",
    "    seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned]), maxlen=max_len, padding='post')\n",
    "    src_tensor = torch.LongTensor(seq).to(device)\n",
    "\n",
    "    start_token = tgt_tokenizer.word_index['<start>']\n",
    "    end_token   = tgt_tokenizer.word_index['<end>']\n",
    "    tgt_tokens  = [start_token]\n",
    "    decoded     = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.LongTensor([tgt_tokens]).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(src_tensor, tgt_tensor)\n",
    "            logits = out[0, -1, :]\n",
    "            next_token = torch.argmax(logits).item()\n",
    "\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "        decoded.append(idx2word.get(next_token, '<unk>'))\n",
    "        tgt_tokens.append(next_token)\n",
    "\n",
    "    return ' '.join(decoded)\n",
    "\n",
    "# --- TEST PREDICTIONS ---\n",
    "test_sentences = [\n",
    "    \"toi yeu tieng viet\",\n",
    "    \"chung ta se chien thang\",\n",
    "    \"ha noi la thu do cua viet nam\"\n",
    "]\n",
    "print(\"Kết quả dự đoán:\")\n",
    "for sent in test_sentences:\n",
    "    print(f\"Input: {sent}\\nOutput: {greedy_decode(model, sent, src_tokenizer, tgt_tokenizer, idx2word)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd7e9d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:48.709249Z",
     "iopub.status.busy": "2025-06-28T14:55:48.708939Z",
     "iopub.status.idle": "2025-06-28T14:55:49.686626Z",
     "shell.execute_reply": "2025-06-28T14:55:49.685822Z"
    },
    "papermill": {
     "duration": 5.076951,
     "end_time": "2025-06-28T14:55:49.687732",
     "exception": false,
     "start_time": "2025-06-28T14:55:44.610781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kết quả dự đoán:\n",
      "Input: toi yeu tieng viet\n",
      "Output: tôi yêu tiếng việt\n",
      "\n",
      "Input: chung ta se chien thang\n",
      "Output: chúng ta sẽ chiến thắng\n",
      "\n",
      "Input: ha noi la thu do cua viet nam\n",
      "Output: hà nội là thủ đô của việt nam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, beam_width=3, max_len=70):\n",
    "    model.eval()\n",
    "\n",
    "    # --- Clean and encode input ---\n",
    "    cleaned_input = input_sentence.strip().lower()\n",
    "    input_seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned_input]), maxlen=max_len, padding='post')\n",
    "    input_tensor = torch.LongTensor(input_seq).to(device)\n",
    "\n",
    "    start_token = tgt_tokenizer.word_index.get('<start>', 1)\n",
    "    end_token = tgt_tokenizer.word_index.get('<end>', 2)\n",
    "\n",
    "    sequences = [[start_token]]\n",
    "    scores = [0.0]\n",
    "    completed_sequences = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in zip(sequences, scores):\n",
    "            if seq[-1] == end_token:\n",
    "                completed_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            tgt_tensor = torch.LongTensor([seq]).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor, tgt_tensor)\n",
    "                logits = output[0, -1, :]\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
    "            for j in range(beam_width):\n",
    "                next_token = topk_indices[j].item()\n",
    "                next_score = score + topk_log_probs[j].item()\n",
    "                all_candidates.append((seq + [next_token], next_score))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "        sequences = [seq for seq, _ in ordered[:beam_width]]\n",
    "        scores = [score for _, score in ordered[:beam_width]]\n",
    "\n",
    "        if all(seq[-1] == end_token for seq in sequences):\n",
    "            break\n",
    "\n",
    "    if completed_sequences:\n",
    "        best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n",
    "    else:\n",
    "        best_seq = sequences[0]\n",
    "\n",
    "    decoded = []\n",
    "    for token in best_seq[1:]:\n",
    "        if token == end_token:\n",
    "            break\n",
    "        decoded.append(idx2word.get(token, '<unk>'))\n",
    "\n",
    "    return ' '.join(decoded)\n",
    "\n",
    "# --- Example test ---\n",
    "test_sentences = [\n",
    "    \"toi yeu tieng viet\",\n",
    "    \"chung ta se chien thang\",\n",
    "    \"ha noi la thu do cua viet nam\"\n",
    "]\n",
    "\n",
    "print(\"\\nKết quả dự đoán:\")\n",
    "for sent in test_sentences:\n",
    "    print(\"Input:\", sent)\n",
    "    print(\"Output:\", beam_search_decode(model, sent, src_tokenizer, tgt_tokenizer, {v:k for k,v in tgt_tokenizer.word_index.items()}, beam_width=5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5693417b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:57.951775Z",
     "iopub.status.busy": "2025-06-28T14:55:57.951491Z",
     "iopub.status.idle": "2025-06-28T14:56:06.754139Z",
     "shell.execute_reply": "2025-06-28T14:56:06.753029Z"
    },
    "papermill": {
     "duration": 12.969407,
     "end_time": "2025-06-28T14:56:06.755754",
     "exception": false,
     "start_time": "2025-06-28T14:55:53.786347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\r\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n",
      "Installing collected packages: portalocker, sacrebleu\r\n",
      "Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a22a6036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:56:14.848350Z",
     "iopub.status.busy": "2025-06-28T14:56:14.847351Z",
     "iopub.status.idle": "2025-06-28T14:56:15.029638Z",
     "shell.execute_reply": "2025-06-28T14:56:15.029005Z"
    },
    "papermill": {
     "duration": 4.323023,
     "end_time": "2025-06-28T14:56:15.031047",
     "exception": false,
     "start_time": "2025-06-28T14:56:10.708024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Advanced BLEU Evaluation Function (Optimized and Precise) ---\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate_bleu(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating BLEU\"):\n",
    "        input_sentence = row['no_diacritics_clean']\n",
    "        reference = row['with_diacritics_clean']\n",
    "\n",
    "        # Clean target (remove <start> and <end>)\n",
    "        reference = reference.replace('<start>', '').replace('<end>', '').strip()\n",
    "\n",
    "        # Decode prediction from model\n",
    "        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n",
    "        prediction = prediction.strip()\n",
    "\n",
    "        references.append([reference])\n",
    "        hypotheses.append(prediction)\n",
    "\n",
    "    # Compute BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, list(map(list, zip(*references))))\n",
    "    print(f\"\\nFinal BLEU Score: {bleu.score:.2f}\")\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b36088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:56:23.075075Z",
     "iopub.status.busy": "2025-06-28T14:56:23.074038Z",
     "iopub.status.idle": "2025-06-28T15:17:07.902235Z",
     "shell.execute_reply": "2025-06-28T15:17:07.901375Z"
    },
    "papermill": {
     "duration": 1248.907435,
     "end_time": "2025-06-28T15:17:07.904180",
     "exception": false,
     "start_time": "2025-06-28T14:56:18.996745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 10000/10000 [20:43<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final BLEU Score: 80.77\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_bleu(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2a1ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:17:16.568124Z",
     "iopub.status.busy": "2025-06-28T15:17:16.567817Z",
     "iopub.status.idle": "2025-06-28T16:23:24.998571Z",
     "shell.execute_reply": "2025-06-28T16:23:24.997651Z"
    },
    "papermill": {
     "duration": 3972.774569,
     "end_time": "2025-06-28T16:23:24.999807",
     "exception": false,
     "start_time": "2025-06-28T15:17:12.225238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 10000/10000 [1:06:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final BLEU Score: 81.31\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_bleu(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf8b1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T16:23:34.556123Z",
     "iopub.status.busy": "2025-06-28T16:23:34.555386Z",
     "iopub.status.idle": "2025-06-28T16:23:34.561881Z",
     "shell.execute_reply": "2025-06-28T16:23:34.561155Z"
    },
    "papermill": {
     "duration": 4.628008,
     "end_time": "2025-06-28T16:23:34.563161",
     "exception": false,
     "start_time": "2025-06-28T16:23:29.935153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Advanced ChrF++ Evaluation Function ---\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_chrf(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating ChrF++\"):\n",
    "        input_sentence = row['no_diacritics_clean']\n",
    "        reference = row['with_diacritics_clean'].replace('<start>', '').replace('<end>', '').strip()\n",
    "\n",
    "        # Decode prediction using the provided decode_fn\n",
    "        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n",
    "        prediction = prediction.strip()  # already stripped from <start>/<end> if decode_fn is correct\n",
    "\n",
    "        # Append for evaluation\n",
    "        references.append([reference])  # list of references for each sentence\n",
    "        hypotheses.append(prediction)   # single prediction per sentence\n",
    "\n",
    "    # Compute ChrF++ score\n",
    "    chrf = sacrebleu.corpus_chrf(hypotheses, list(map(list, zip(*references))))\n",
    "    print(f\"\\nFinal ChrF++ Score: {chrf.score:.2f}\")\n",
    "    return chrf.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd363081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T16:23:44.138569Z",
     "iopub.status.busy": "2025-06-28T16:23:44.137845Z",
     "iopub.status.idle": "2025-06-28T16:44:59.796391Z",
     "shell.execute_reply": "2025-06-28T16:44:59.795401Z"
    },
    "papermill": {
     "duration": 1280.335335,
     "end_time": "2025-06-28T16:44:59.797911",
     "exception": false,
     "start_time": "2025-06-28T16:23:39.462576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ChrF++: 100%|██████████| 10000/10000 [21:12<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ChrF++ Score: 88.41\n"
     ]
    }
   ],
   "source": [
    "chrf_score = evaluate_chrf(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74bd8cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T16:45:09.888553Z",
     "iopub.status.busy": "2025-06-28T16:45:09.887769Z",
     "iopub.status.idle": "2025-06-28T17:51:37.017534Z",
     "shell.execute_reply": "2025-06-28T17:51:37.016714Z"
    },
    "papermill": {
     "duration": 3992.266861,
     "end_time": "2025-06-28T17:51:37.018738",
     "exception": false,
     "start_time": "2025-06-28T16:45:04.751877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ChrF++: 100%|██████████| 10000/10000 [1:06:24<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ChrF++ Score: 88.57\n"
     ]
    }
   ],
   "source": [
    "chrf_score = evaluate_chrf(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc29eea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T17:51:47.585875Z",
     "iopub.status.busy": "2025-06-28T17:51:47.585140Z",
     "iopub.status.idle": "2025-06-28T17:51:47.588837Z",
     "shell.execute_reply": "2025-06-28T17:51:47.588331Z"
    },
    "papermill": {
     "duration": 5.301742,
     "end_time": "2025-06-28T17:51:47.589911",
     "exception": false,
     "start_time": "2025-06-28T17:51:42.288169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import sacrebleu\n",
    "\n",
    "# # --- Load tokenizers ---\n",
    "# with open(\"/kaggle/input/transformer-seq2seq-diacritics/transformers/default/1/src_tokenizer.pkl\", \"rb\") as f:\n",
    "#     src_tokenizer = pickle.load(f)\n",
    "\n",
    "# with open(\"/kaggle/input/transformer-seq2seq-diacritics/transformers/default/1/tgt_tokenizer.pkl\", \"rb\") as f:\n",
    "#     tgt_tokenizer = pickle.load(f)\n",
    "\n",
    "# # --- Load model ---\n",
    "# model_path = \"/kaggle/input/transformer-seq2seq-diacritics/transformers/default/1/transformer_model.pt\"\n",
    "# model = Transformer(len(src_tokenizer.word_index)+1, len(tgt_tokenizer.word_index)+1)\n",
    "# model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Create idx2word mapping ---\n",
    "# idx2word = {idx: word for word, idx in tgt_tokenizer.word_index.items()}"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7685807,
     "sourceId": 12201813,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 386794,
     "modelInstanceId": 365901,
     "sourceId": 451014,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25891.297717,
   "end_time": "2025-06-28T17:51:55.901373",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-28T10:40:24.603656",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
