{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6d249b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:40:30.749559Z",
     "iopub.status.busy": "2025-06-28T10:40:30.749279Z",
     "iopub.status.idle": "2025-06-28T10:41:01.929213Z",
     "shell.execute_reply": "2025-06-28T10:41:01.928296Z"
    },
    "papermill": {
     "duration": 31.186605,
     "end_time": "2025-06-28T10:41:01.930901",
     "exception": false,
     "start_time": "2025-06-28T10:40:30.744296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 10:40:43.178786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751107243.584917      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751107243.691809      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f368fa0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:41:01.940041Z",
     "iopub.status.busy": "2025-06-28T10:41:01.938999Z",
     "iopub.status.idle": "2025-06-28T10:41:02.039755Z",
     "shell.execute_reply": "2025-06-28T10:41:02.039133Z"
    },
    "papermill": {
     "duration": 0.106399,
     "end_time": "2025-06-28T10:41:02.041182",
     "exception": false,
     "start_time": "2025-06-28T10:41:01.934783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- DEVICE SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c546e6bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:41:02.048857Z",
     "iopub.status.busy": "2025-06-28T10:41:02.048404Z",
     "iopub.status.idle": "2025-06-28T10:42:24.893384Z",
     "shell.execute_reply": "2025-06-28T10:42:24.892488Z"
    },
    "papermill": {
     "duration": 82.850369,
     "end_time": "2025-06-28T10:42:24.894973",
     "exception": false,
     "start_time": "2025-06-28T10:41:02.044604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- LOAD DATA ---\n",
    "path = \"/kaggle/input/vietnamese-diacritics-dataset/\"\n",
    "df_train = pd.read_csv(path + \"ViDiacritics_train.csv\")\n",
    "df_val   = pd.read_csv(path + \"ViDiacritics_val.csv\")\n",
    "df_test  = pd.read_csv(path + \"ViDiacritics_test.csv\")\n",
    "\n",
    "df_train = df_train.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "df_val   = df_val.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "df_test  = df_test.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372f25a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:42:24.907240Z",
     "iopub.status.busy": "2025-06-28T10:42:24.906565Z",
     "iopub.status.idle": "2025-06-28T10:42:54.684038Z",
     "shell.execute_reply": "2025-06-28T10:42:54.683140Z"
    },
    "papermill": {
     "duration": 29.787004,
     "end_time": "2025-06-28T10:42:54.685679",
     "exception": false,
     "start_time": "2025-06-28T10:42:24.898675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CLEAN TEXT ---\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def apply_cleaning(df):\n",
    "    df['no_diacritics_clean']   = df['no_diacritics'].astype(str).apply(clean_text)\n",
    "    df['with_diacritics_clean'] = df['with_diacritics'].astype(str).apply(clean_text)\n",
    "    df['with_diacritics_clean'] = df['with_diacritics_clean'].apply(lambda x: '<start> ' + x + ' <end>')\n",
    "    return df\n",
    "\n",
    "df_train = apply_cleaning(df_train)\n",
    "df_val   = apply_cleaning(df_val)\n",
    "df_test  = apply_cleaning(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5317de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:42:54.694299Z",
     "iopub.status.busy": "2025-06-28T10:42:54.693672Z",
     "iopub.status.idle": "2025-06-28T10:43:32.619831Z",
     "shell.execute_reply": "2025-06-28T10:43:32.619005Z"
    },
    "papermill": {
     "duration": 37.931782,
     "end_time": "2025-06-28T10:43:32.621461",
     "exception": false,
     "start_time": "2025-06-28T10:42:54.689679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- TOKENIZATION ---\n",
    "tokenizer_filters = '\"!#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # giá»¯ <>\n",
    "src_tokenizer = Tokenizer(oov_token='<unk>', filters=tokenizer_filters)\n",
    "tgt_tokenizer = Tokenizer(oov_token='<unk>', filters=tokenizer_filters)\n",
    "src_tokenizer.fit_on_texts(df_train['no_diacritics_clean'])\n",
    "tgt_tokenizer.fit_on_texts(df_train['with_diacritics_clean'])\n",
    "\n",
    "SRC_VOCAB_SIZE = len(src_tokenizer.word_index) + 1\n",
    "TGT_VOCAB_SIZE = len(tgt_tokenizer.word_index) + 1\n",
    "MAX_LEN = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02a5e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:43:32.629238Z",
     "iopub.status.busy": "2025-06-28T10:43:32.629011Z",
     "iopub.status.idle": "2025-06-28T10:44:13.595832Z",
     "shell.execute_reply": "2025-06-28T10:44:13.595240Z"
    },
    "papermill": {
     "duration": 40.972209,
     "end_time": "2025-06-28T10:44:13.597284",
     "exception": false,
     "start_time": "2025-06-28T10:43:32.625075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ENCODING ---\n",
    "def encode_and_pad(texts, tokenizer):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "train_src = encode_and_pad(df_train['no_diacritics_clean'], src_tokenizer)\n",
    "train_tgt = encode_and_pad(df_train['with_diacritics_clean'], tgt_tokenizer)\n",
    "val_src   = encode_and_pad(df_val['no_diacritics_clean'], src_tokenizer)\n",
    "val_tgt   = encode_and_pad(df_val['with_diacritics_clean'], tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ca6add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:44:13.605523Z",
     "iopub.status.busy": "2025-06-28T10:44:13.605030Z",
     "iopub.status.idle": "2025-06-28T10:44:14.189379Z",
     "shell.execute_reply": "2025-06-28T10:44:14.188662Z"
    },
    "papermill": {
     "duration": 0.589889,
     "end_time": "2025-06-28T10:44:14.190882",
     "exception": false,
     "start_time": "2025-06-28T10:44:13.600993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- DATASET ---\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = torch.LongTensor(src)\n",
    "        self.tgt = torch.LongTensor(tgt)\n",
    "    def __len__(self): return len(self.src)\n",
    "    def __getitem__(self, idx): return self.src[idx], self.tgt[idx]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(TranslationDataset(train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TranslationDataset(val_src, val_tgt), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce19e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:44:14.199453Z",
     "iopub.status.busy": "2025-06-28T10:44:14.199049Z",
     "iopub.status.idle": "2025-06-28T10:44:14.219414Z",
     "shell.execute_reply": "2025-06-28T10:44:14.218841Z"
    },
    "papermill": {
     "duration": 0.025793,
     "end_time": "2025-06-28T10:44:14.220454",
     "exception": false,
     "start_time": "2025-06-28T10:44:14.194661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- TRANSFORMER MODULES ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        Q, K, V = Q.float(), K.float(), V.float()\n",
    "        scores = Q.matmul(K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        return attn.matmul(V)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        b, seq_len, _ = x.size()\n",
    "        return x.view(b, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        b, h, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(b, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        out = self.scaled_dot_product(Q, K, V, mask)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.W_o(out)\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff   = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1= nn.LayerNorm(d_model)\n",
    "        self.norm2= nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.norm1(x + self.dropout(self.attn(x,x,x,mask)))\n",
    "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn  = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff         = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1      = nn.LayerNorm(d_model)\n",
    "        self.norm2      = nn.LayerNorm(d_model)\n",
    "        self.norm3      = nn.LayerNorm(d_model)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x,x,x,tgt_mask)))\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(x,enc_out,enc_out,src_mask)))\n",
    "        x = self.norm3(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8, num_layers=2, d_ff=512, max_len=70, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        seq_len = tgt.size(1)\n",
    "        nopeak = torch.tril(torch.ones(1,1,seq_len,seq_len, device=tgt.device)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        x = self.dropout(self.pos_enc(self.enc_emb(src)))\n",
    "        y = self.dropout(self.pos_enc(self.dec_emb(tgt)))\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        for layer in self.dec_layers:\n",
    "            y = layer(y, x, src_mask, tgt_mask)\n",
    "        return self.fc_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "095fbcae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T10:44:14.227769Z",
     "iopub.status.busy": "2025-06-28T10:44:14.227563Z",
     "iopub.status.idle": "2025-06-28T14:55:22.142210Z",
     "shell.execute_reply": "2025-06-28T14:55:22.141289Z"
    },
    "papermill": {
     "duration": 15067.920178,
     "end_time": "2025-06-28T14:55:22.143829",
     "exception": false,
     "start_time": "2025-06-28T10:44:14.223651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1068574685.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n",
      "Epoch 1 Training:   0%|          | 0/15688 [00:00<?, ?it/s]/tmp/ipykernel_19/1068574685.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Training: 100%|ââââââââââ| 15688/15688 [46:09<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.1965 | Acc: 0.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|ââââââââââ| 1961/1961 [04:01<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.5842 | Acc: 0.8869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|ââââââââââ| 15688/15688 [46:16<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.6247 | Acc: 0.8731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|ââââââââââ| 1961/1961 [04:02<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.4819 | Acc: 0.9063\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|ââââââââââ| 15688/15688 [46:04<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.5220 | Acc: 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|ââââââââââ| 1961/1961 [04:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.4305 | Acc: 0.9161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|ââââââââââ| 15688/15688 [46:04<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.4614 | Acc: 0.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|ââââââââââ| 1961/1961 [04:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.3919 | Acc: 0.9239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|ââââââââââ| 15688/15688 [46:15<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.4276 | Acc: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|ââââââââââ| 1961/1961 [04:01<00:00,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Loss: 0.3758 | Acc: 0.9272\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL & TRAINING ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "model     = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "scaler    = GradScaler()\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, total_tokens, total_correct = 0, 0, 0\n",
    "    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits = model(src, tgt_input).view(-1, TGT_VOCAB_SIZE)\n",
    "            tgt_flat = tgt_output.reshape(-1)\n",
    "            loss = criterion(logits, tgt_flat)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        mask = tgt_flat != 0\n",
    "        total_loss   += loss.item() * mask.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct+= (preds == tgt_flat).masked_select(mask).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = total_loss / total_tokens\n",
    "    train_acc  = total_correct / total_tokens\n",
    "    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_tokens, val_correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            logits = model(src, tgt_input).view(-1, TGT_VOCAB_SIZE)\n",
    "            tgt_flat = tgt_output.reshape(-1)\n",
    "            loss = criterion(logits, tgt_flat)\n",
    "            mask = tgt_flat != 0\n",
    "            val_loss   += loss.item() * mask.sum().item()\n",
    "            val_tokens += mask.sum().item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct+= (preds == tgt_flat).masked_select(mask).sum().item()\n",
    "    print(f\"[Val] Loss: {val_loss/val_tokens:.4f} | Acc: {val_correct/val_tokens:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703bba16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:30.344438Z",
     "iopub.status.busy": "2025-06-28T14:55:30.343511Z",
     "iopub.status.idle": "2025-06-28T14:55:31.982552Z",
     "shell.execute_reply": "2025-06-28T14:55:31.981583Z"
    },
    "papermill": {
     "duration": 5.896204,
     "end_time": "2025-06-28T14:55:31.983926",
     "exception": false,
     "start_time": "2025-06-28T14:55:26.087722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_model.pt\n",
      "Model package zipped to transformer_model_package.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "# TÃªn file lÆ°u model\n",
    "model_filename = \"transformer_model.pt\"\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# LÆ°u tokenizer (dÃ¹ng pickle hoáº·c joblib)\n",
    "import pickle\n",
    "with open(\"src_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(src_tokenizer, f)\n",
    "with open(\"tgt_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tgt_tokenizer, f)\n",
    "\n",
    "# Táº¡o danh sÃ¡ch file cáº§n zip\n",
    "files_to_zip = [\n",
    "    \"transformer_model.pt\",\n",
    "    \"src_tokenizer.pkl\",\n",
    "    \"tgt_tokenizer.pkl\"\n",
    "]\n",
    "\n",
    "# NÃ©n thÃ nh file zip\n",
    "zip_filename = \"transformer_model_package.zip\"\n",
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    for file in files_to_zip:\n",
    "        if os.path.exists(file):\n",
    "            zipf.write(file)\n",
    "\n",
    "print(f\"Model package zipped to {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a5f335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:40.071135Z",
     "iopub.status.busy": "2025-06-28T14:55:40.070316Z",
     "iopub.status.idle": "2025-06-28T14:55:40.626667Z",
     "shell.execute_reply": "2025-06-28T14:55:40.625657Z"
    },
    "papermill": {
     "duration": 4.696125,
     "end_time": "2025-06-28T14:55:40.627915",
     "exception": false,
     "start_time": "2025-06-28T14:55:35.931790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Káº¿t quáº£ dá»± ÄoÃ¡n:\n",
      "Input: toi yeu tieng viet\n",
      "Output: tÃ´i yÃªu tiáº¿ng viá»t\n",
      "\n",
      "Input: chung ta se chien thang\n",
      "Output: chÃºng ta sáº½ chiáº¿n tháº¯ng\n",
      "\n",
      "Input: ha noi la thu do cua viet nam\n",
      "Output: hÃ  ná»i lÃ  thá»§ ÄÃ´ cá»§a viá»t nam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UTILITY FUNCTIONS ---\n",
    "idx2word = {idx:word for word,idx in tgt_tokenizer.word_index.items()}\n",
    "idx2word[0] = '<pad>'\n",
    "\n",
    "def greedy_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=70):\n",
    "    model.eval()\n",
    "    cleaned = clean_text(input_sentence)\n",
    "    seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned]), maxlen=max_len, padding='post')\n",
    "    src_tensor = torch.LongTensor(seq).to(device)\n",
    "\n",
    "    start_token = tgt_tokenizer.word_index['<start>']\n",
    "    end_token   = tgt_tokenizer.word_index['<end>']\n",
    "    tgt_tokens  = [start_token]\n",
    "    decoded     = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.LongTensor([tgt_tokens]).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(src_tensor, tgt_tensor)\n",
    "            logits = out[0, -1, :]\n",
    "            next_token = torch.argmax(logits).item()\n",
    "\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "        decoded.append(idx2word.get(next_token, '<unk>'))\n",
    "        tgt_tokens.append(next_token)\n",
    "\n",
    "    return ' '.join(decoded)\n",
    "\n",
    "# --- TEST PREDICTIONS ---\n",
    "test_sentences = [\n",
    "    \"toi yeu tieng viet\",\n",
    "    \"chung ta se chien thang\",\n",
    "    \"ha noi la thu do cua viet nam\"\n",
    "]\n",
    "print(\"Káº¿t quáº£ dá»± ÄoÃ¡n:\")\n",
    "for sent in test_sentences:\n",
    "    print(f\"Input: {sent}\\nOutput: {greedy_decode(model, sent, src_tokenizer, tgt_tokenizer, idx2word)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd7e9d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:48.709249Z",
     "iopub.status.busy": "2025-06-28T14:55:48.708939Z",
     "iopub.status.idle": "2025-06-28T14:55:49.686626Z",
     "shell.execute_reply": "2025-06-28T14:55:49.685822Z"
    },
    "papermill": {
     "duration": 5.076951,
     "end_time": "2025-06-28T14:55:49.687732",
     "exception": false,
     "start_time": "2025-06-28T14:55:44.610781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Káº¿t quáº£ dá»± ÄoÃ¡n:\n",
      "Input: toi yeu tieng viet\n",
      "Output: tÃ´i yÃªu tiáº¿ng viá»t\n",
      "\n",
      "Input: chung ta se chien thang\n",
      "Output: chÃºng ta sáº½ chiáº¿n tháº¯ng\n",
      "\n",
      "Input: ha noi la thu do cua viet nam\n",
      "Output: hÃ  ná»i lÃ  thá»§ ÄÃ´ cá»§a viá»t nam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decode(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, beam_width=3, max_len=70):\n",
    "    model.eval()\n",
    "\n",
    "    # --- Clean and encode input ---\n",
    "    cleaned_input = input_sentence.strip().lower()\n",
    "    input_seq = pad_sequences(src_tokenizer.texts_to_sequences([cleaned_input]), maxlen=max_len, padding='post')\n",
    "    input_tensor = torch.LongTensor(input_seq).to(device)\n",
    "\n",
    "    start_token = tgt_tokenizer.word_index.get('<start>', 1)\n",
    "    end_token = tgt_tokenizer.word_index.get('<end>', 2)\n",
    "\n",
    "    sequences = [[start_token]]\n",
    "    scores = [0.0]\n",
    "    completed_sequences = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in zip(sequences, scores):\n",
    "            if seq[-1] == end_token:\n",
    "                completed_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            tgt_tensor = torch.LongTensor([seq]).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor, tgt_tensor)\n",
    "                logits = output[0, -1, :]\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
    "            for j in range(beam_width):\n",
    "                next_token = topk_indices[j].item()\n",
    "                next_score = score + topk_log_probs[j].item()\n",
    "                all_candidates.append((seq + [next_token], next_score))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "        sequences = [seq for seq, _ in ordered[:beam_width]]\n",
    "        scores = [score for _, score in ordered[:beam_width]]\n",
    "\n",
    "        if all(seq[-1] == end_token for seq in sequences):\n",
    "            break\n",
    "\n",
    "    if completed_sequences:\n",
    "        best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n",
    "    else:\n",
    "        best_seq = sequences[0]\n",
    "\n",
    "    decoded = []\n",
    "    for token in best_seq[1:]:\n",
    "        if token == end_token:\n",
    "            break\n",
    "        decoded.append(idx2word.get(token, '<unk>'))\n",
    "\n",
    "    return ' '.join(decoded)\n",
    "\n",
    "# --- Example test ---\n",
    "test_sentences = [\n",
    "    \"toi yeu tieng viet\",\n",
    "    \"chung ta se chien thang\",\n",
    "    \"ha noi la thu do cua viet nam\"\n",
    "]\n",
    "\n",
    "print(\"\\nKáº¿t quáº£ dá»± ÄoÃ¡n:\")\n",
    "for sent in test_sentences:\n",
    "    print(\"Input:\", sent)\n",
    "    print(\"Output:\", beam_search_decode(model, sent, src_tokenizer, tgt_tokenizer, {v:k for k,v in tgt_tokenizer.word_index.items()}, beam_width=5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5693417b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:55:57.951775Z",
     "iopub.status.busy": "2025-06-28T14:55:57.951491Z",
     "iopub.status.idle": "2025-06-28T14:56:06.754139Z",
     "shell.execute_reply": "2025-06-28T14:56:06.753029Z"
    },
    "papermill": {
     "duration": 12.969407,
     "end_time": "2025-06-28T14:56:06.755754",
     "exception": false,
     "start_time": "2025-06-28T14:55:53.786347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\r\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n",
      "Installing collected packages: portalocker, sacrebleu\r\n",
      "Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a22a6036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:56:14.848350Z",
     "iopub.status.busy": "2025-06-28T14:56:14.847351Z",
     "iopub.status.idle": "2025-06-28T14:56:15.029638Z",
     "shell.execute_reply": "2025-06-28T14:56:15.029005Z"
    },
    "papermill": {
     "duration": 4.323023,
     "end_time": "2025-06-28T14:56:15.031047",
     "exception": false,
     "start_time": "2025-06-28T14:56:10.708024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Advanced BLEU Evaluation Function (Optimized and Precise) ---\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate_bleu(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating BLEU\"):\n",
    "        input_sentence = row['no_diacritics_clean']\n",
    "        reference = row['with_diacritics_clean']\n",
    "\n",
    "        # Clean target (remove <start> and <end>)\n",
    "        reference = reference.replace('<start>', '').replace('<end>', '').strip()\n",
    "\n",
    "        # Decode prediction from model\n",
    "        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n",
    "        prediction = prediction.strip()\n",
    "\n",
    "        references.append([reference])\n",
    "        hypotheses.append(prediction)\n",
    "\n",
    "    # Compute BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, list(map(list, zip(*references))))\n",
    "    print(f\"\\nFinal BLEU Score: {bleu.score:.2f}\")\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b36088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:56:23.075075Z",
     "iopub.status.busy": "2025-06-28T14:56:23.074038Z",
     "iopub.status.idle": "2025-06-28T15:17:07.902235Z",
     "shell.execute_reply": "2025-06-28T15:17:07.901375Z"
    },
    "papermill": {
     "duration": 1248.907435,
     "end_time": "2025-06-28T15:17:07.904180",
     "exception": false,
     "start_time": "2025-06-28T14:56:18.996745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|ââââââââââ| 10000/10000 [20:43<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final BLEU Score: 80.77\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_bleu(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2a1ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:17:16.568124Z",
     "iopub.status.busy": "2025-06-28T15:17:16.567817Z",
     "iopub.status.idle": "2025-06-28T16:23:24.998571Z",
     "shell.execute_reply": "2025-06-28T16:23:24.997651Z"
    },
    "papermill": {
     "duration": 3972.774569,
     "end_time": "2025-06-28T16:23:24.999807",
     "exception": false,
     "start_time": "2025-06-28T15:17:12.225238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|ââââââââââ| 10000/10000 [1:06:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final BLEU Score: 81.31\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_bleu(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf8b1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T16:23:34.556123Z",
     "iopub.status.busy": "2025-06-28T16:23:34.555386Z",
     "iopub.status.idle": "2025-06-28T16:23:34.561881Z",
     "shell.execute_reply": "2025-06-28T16:23:34.561155Z"
    },
    "papermill": {
     "duration": 4.628008,
     "end_time": "2025-06-28T16:23:34.563161",
     "exception": false,
     "start_time": "2025-06-28T16:23:29.935153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Advanced ChrF++ Evaluation Function ---\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_chrf(model, df, src_tokenizer, tgt_tokenizer, idx2word, decode_fn, max_len=70):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating ChrF++\"):\n",
    "        input_sentence = row['no_diacritics_clean']\n",
    "        reference = row['with_diacritics_clean'].replace('<start>', '').replace('<end>', '').strip()\n",
    "\n",
    "        # Decode prediction using the provided decode_fn\n",
    "        prediction = decode_fn(model, input_sentence, src_tokenizer, tgt_tokenizer, idx2word, max_len=max_len)\n",
    "        prediction = prediction.strip()  # already stripped from <start>/<end> if decode_fn is correct\n",
    "\n",
    "        # Append for evaluation\n",
    "        references.append([reference])  # list of references for each sentence\n",
    "        hypotheses.append(prediction)   # single prediction per sentence\n",
    "\n",
    "    # Compute ChrF++ score\n",
    "    chrf = sacrebleu.corpus_chrf(hypotheses, list(map(list, zip(*references))))\n",
    "    print(f\"\\nFinal ChrF++ Score: {chrf.score:.2f}\")\n",
    "    return chrf.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd363081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T16:23:44.138569Z",
     "iopub.status.busy": "2025-06-28T16:23:44.137845Z",
     "iopub.status.idle": "2025-06-28T16:44:59.796391Z",
     "shell.execute_reply": "2025-06-28T16:44:59.795401Z"
    },
    "papermill": {
     "duration": 1280.335335,
     "end_time": "2025-06-28T16:44:59.797911",
     "exception": false,
     "start_time": "2025-06-28T16:23:39.462576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ChrF++: 100%|ââââââââââ| 10000/10000 [21:12<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ChrF++ Score: 88.41\n"
     ]
    }
   ],
   "source": [
    "chrf_score = evaluate_chrf(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=greedy_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74bd8cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T16:45:09.888553Z",
     "iopub.status.busy": "2025-06-28T16:45:09.887769Z",
     "iopub.status.idle": "2025-06-28T17:51:37.017534Z",
     "shell.execute_reply": "2025-06-28T17:51:37.016714Z"
    },
    "papermill": {
     "duration": 3992.266861,
     "end_time": "2025-06-28T17:51:37.018738",
     "exception": false,
     "start_time": "2025-06-28T16:45:04.751877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ChrF++: 100%|ââââââââââ| 10000/10000 [1:06:24<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ChrF++ Score: 88.57\n"
     ]
    }
   ],
   "source": [
    "chrf_score = evaluate_chrf(model, df_test.sample(10000), src_tokenizer, tgt_tokenizer, idx2word, decode_fn=beam_search_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc29eea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T17:51:47.585875Z",
     "iopub.status.busy": "2025-06-28T17:51:47.585140Z",
     "iopub.status.idle": "2025-06-28T17:51:47.588837Z",
     "shell.execute_reply": "2025-06-28T17:51:47.588331Z"
    },
    "papermill": {
     "duration": 5.301742,
     "end_time": "2025-06-28T17:51:47.589911",
     "exception": false,
     "start_time": "2025-06-28T17:51:42.288169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import sacrebleu\n",
    "\n",
    "# # --- Load tokenizers ---\n",
    "# with open(\"/kaggle/input/transformer-seq2seq-diacritics/transformers/default/1/src_tokenizer.pkl\", \"rb\") as f:\n",
    "#     src_tokenizer = pickle.load(f)\n",
    "\n",
    "# with open(\"/kaggle/input/transformer-seq2seq-diacritics/transformers/default/1/tgt_tokenizer.pkl\", \"rb\") as f:\n",
    "#     tgt_tokenizer = pickle.load(f)\n",
    "\n",
    "# # --- Load model ---\n",
    "# model_path = \"/kaggle/input/transformer-seq2seq-diacritics/transformers/default/1/transformer_model.pt\"\n",
    "# model = Transformer(len(src_tokenizer.word_index)+1, len(tgt_tokenizer.word_index)+1)\n",
    "# model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Create idx2word mapping ---\n",
    "# idx2word = {idx: word for word, idx in tgt_tokenizer.word_index.items()}"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7685807,
     "sourceId": 12201813,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 386794,
     "modelInstanceId": 365901,
     "sourceId": 451014,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25891.297717,
   "end_time": "2025-06-28T17:51:55.901373",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-28T10:40:24.603656",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
